{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frozen Lake\n",
    "===\n",
    "The goal of this computer assignment is to get familiar with OpenAI Gym, implement value iteration and policy iteration.\n",
    "\n",
    "Problem Description\n",
    "---\n",
    "OpenAI Gym is a toolkit for developing and comparing reinforcement learning algorithms. It supports teaching agents everything from walking to playing games like Pong or Pinball. For more information visit https://gym.openai.com.\n",
    "\n",
    "In this computer assigment, you'll get familiar with Frozen Lake environment and implement value and policy iteration algorithms. Frozen Lake is an environment where the agent controls the movement of a character in a grid world. Some tiles of the grid are walkable, and others lead to the agent falling into the water. Additionally, the movement direction of the agent is uncertain and only partially depends on the chosen direction. The agent is rewarded for finding a walkable path to a goal tile. For more information please visit https://gym.openai.com/envs/FrozenLake8x8-v0/.\n",
    "\n",
    "Your Job\n",
    "---\n",
    "1. Get started with gym by following the steps here https://gym.openai.com/docs/.\n",
    "2. Read https://gym.openai.com/envs/FrozenLake8x8-v0/ to get familiar with the environment, states, reward function, etc.\n",
    "3. Implement the $\\texttt{value_iteration}$ function below.\n",
    "4. Implement the $\\texttt{policy_iteration}$ function below.\n",
    "5. Answer the questions (By double click on the cell you can edit the cell and put your answer below each question)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Gamma=0.9 ----------\n",
      "Total iterations needed for convergence were : 315\n",
      "Value iteration took 0.3606681823730469 seconds.\n",
      "Average score =  0.0060644523272935565\n",
      "---------- Gamma=0.95 ----------\n",
      "Total iterations needed for convergence were : 495\n",
      "Value iteration took 0.6319043636322021 seconds.\n",
      "Average score =  0.048616381696941935\n",
      "---------- Gamma=0.99 ----------\n",
      "Total iterations needed for convergence were : 1128\n",
      "Value iteration took 1.4338624477386475 seconds.\n",
      "Average score =  0.40879407428412834\n",
      "---------- Gamma=0.9999 ----------\n",
      "Total iterations needed for convergence were : 2337\n",
      "Value iteration took 2.8529796600341797 seconds.\n",
      "Average score =  0.8756445024265985\n",
      "---------- Gamma=1 ----------\n",
      "Total iterations needed for convergence were : 2357\n",
      "Value iteration took 3.0036962032318115 seconds.\n",
      "Average score =  0.878\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import time\n",
    "\n",
    "def run_episode(env, policy, gamma, render = False):\n",
    "    \"\"\" Evaluates policy by using it to run an episode and finding its\n",
    "    total reward.\n",
    "    args:\n",
    "    env: gym environment.\n",
    "    policy: the policy to be used.\n",
    "    gamma: discount factor.\n",
    "    render: boolean to turn rendering on/off.\n",
    "    returns:\n",
    "    total reward: real value of the total reward recieved by agent under policy.\n",
    "    \"\"\"\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    step_idx = 0\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()\n",
    "        obs, reward, done , _ = env.step(int(policy[obs]))\n",
    "        total_reward += (gamma ** step_idx * reward)\n",
    "        step_idx += 1\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "def evaluate_policy(env, policy, gamma,  n = 100):\n",
    "    \"\"\" Evaluates a policy by running it n times.\n",
    "    returns:\n",
    "    average total reward\n",
    "    \"\"\"\n",
    "    scores = [\n",
    "            run_episode(env, policy, gamma=gamma, render = False)\n",
    "            for _ in range(n)]\n",
    "    return np.mean(scores)\n",
    "\n",
    "def extract_policy(v, gamma):\n",
    "    \"\"\" Extract the policy given a value-function \"\"\"\n",
    "    policy = np.zeros(env.nS)\n",
    "    for s in range(env.nS):\n",
    "        q_sa = np.zeros(env.action_space.n)\n",
    "        for a in range(env.action_space.n):\n",
    "            for next_sr in env.P[s][a]:\n",
    "                # next_sr is a tuple of (probability, next state, reward, done)\n",
    "                p, s_, r, _ = next_sr\n",
    "                q_sa[a] += (p * (r + gamma * v[s_]))\n",
    "        policy[s] = np.argmax(q_sa)\n",
    "    return policy\n",
    "\n",
    "\n",
    "def value_iteration(env, gamma, epsilon=1e-20, max_iterations=100000):\n",
    "    \"\"\"\n",
    "    This function implements value iteration algorithm for the infinite\n",
    "    horizon discounted MDPs. If the sup norm of v_k - v_{k-1} is less than\n",
    "    epsilon or number of iterations reaches max_iterations, it should return\n",
    "    the value function.\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    v = np.zeros(env.nS)  # initialize value-function\n",
    "    ########################### Your Code Here ###########################\n",
    "    # Hint: see implementation of extract_policy\n",
    "    \n",
    "    for ite in np.arange(max_iterations):\n",
    "    \n",
    "        if ite != 0:\n",
    "            v = v_upd_whole\n",
    "\n",
    "        v_upd_whole = np.zeros(env.nS)\n",
    "        for s in np.arange(env.nS):\n",
    "            v_collec = np.zeros(env.action_space.n)\n",
    "            for a in np.arange(env.action_space.n):\n",
    "                v_loc = 0\n",
    "                for jmps in env.P[s][a]:\n",
    "                    p,s_next,r,_ = jmps\n",
    "                    v_loc = v_loc + p*r + gamma*p*v[s_next]\n",
    "\n",
    "                v_collec[a] = v_loc\n",
    "\n",
    "\n",
    "            v_upd_ind = np.max(v_collec)\n",
    "            v_upd_whole[s] = v_upd_ind\n",
    "            \n",
    "        if LA.norm(v_upd_whole-v,np.inf) < epsilon:\n",
    "            v = v_upd_whole\n",
    "            eff_ite = ite + 1\n",
    "            print('Total iterations needed for convergence were :',eff_ite)\n",
    "            break\n",
    "        \n",
    "        if ite == max_iterations - 1:\n",
    "            v = v_upd_whole\n",
    "            tot_ite = ite + 1\n",
    "            print('Total iterations, without convergence, were the maximum iterations :',tot_ite)\n",
    "\n",
    "       \n",
    "     \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ########################### End of your code #########################\n",
    "    end = time.time()\n",
    "    print(\"Value iteration took {0} seconds.\".format(end - start))\n",
    "    return v\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    np.random.seed(1111)\n",
    "    env_name  = 'FrozenLake8x8-v0'\n",
    "    for gamma in [.9, .95, .99, .9999, 1]:\n",
    "        print(\"-\"*10, \"Gamma={0}\".format(gamma) ,\"-\"*10)\n",
    "        env = gym.make(env_name)\n",
    "        optimal_v = value_iteration(env, gamma);\n",
    "        policy = extract_policy(optimal_v, gamma)\n",
    "        policy_score = evaluate_policy(env, policy, gamma, n=1000)\n",
    "        print('Average score = ', policy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy Iteration\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Gamma=0.9 ----------\n",
      "Total iterations needed for convergence were : 5\n",
      "Policy iteration took 0.2047128677368164 seconds.\n",
      "Average scores =  0.0062415895498807135\n",
      "---------- Gamma=0.95 ----------\n",
      "Total iterations needed for convergence were : 3\n",
      "Policy iteration took 0.2377605438232422 seconds.\n",
      "Average scores =  0.04831133610155477\n",
      "---------- Gamma=0.99 ----------\n",
      "Total iterations needed for convergence were : 8\n",
      "Policy iteration took 1.3672118186950684 seconds.\n",
      "Average scores =  0.3862772551379816\n",
      "---------- Gamma=0.9999 ----------\n",
      "Total iterations needed for convergence were : 12\n",
      "Policy iteration took 4.202902793884277 seconds.\n",
      "Average scores =  0.8814389744647637\n",
      "---------- Gamma=1 ----------\n",
      "Total iterations needed for convergence were : 6\n",
      "Policy iteration took 2.4776272773742676 seconds.\n",
      "Average scores =  0.93\n"
     ]
    }
   ],
   "source": [
    "def compute_policy_v(env, policy, gamma):\n",
    "    \"\"\" Iteratively evaluate the value-function under policy.\n",
    "    Alternatively, we could formulate a set of linear equations in iterms of v[s] \n",
    "    and solve them to find the value function.\n",
    "    \"\"\"\n",
    "    v = np.zeros(env.nS)\n",
    "    eps = 1e-10\n",
    "    while True:\n",
    "        prev_v = np.copy(v)\n",
    "        for s in range(env.nS):\n",
    "            policy_a = policy[s]\n",
    "            v[s] = sum([p * (r + gamma * prev_v[s_]) for p, s_, r, _ in env.P[s][policy_a]])\n",
    "        if (np.sum((np.fabs(prev_v - v))) <= eps):\n",
    "            # value converged\n",
    "            break\n",
    "    return v\n",
    "\n",
    "def policy_iteration(env, gamma, max_iterations=100000):\n",
    "    \"\"\"\n",
    "    This function implements policy iteration algorithm.\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    policy = np.random.choice(env.nA, size=(env.nS))  # initialize a random policy\n",
    "    ########################### Your Code Here ###########################\n",
    "    \n",
    "    \n",
    "    for ite in np.arange(max_iterations):\n",
    "        \n",
    "        v_upd_whole = compute_policy_v(env,policy,gamma)           # only major algorithmic modification\n",
    "        policy_old = np.copy(policy)\n",
    "        v = v_upd_whole\n",
    "        for s in np.arange(env.nS):\n",
    "            v_collec = np.zeros(env.action_space.n)\n",
    "            for a in np.arange(env.action_space.n):\n",
    "                v_loc = 0\n",
    "                for jmps in env.P[s][a]:\n",
    "                    p,s_next,r,_ = jmps\n",
    "                    v_loc = v_loc + p*r + gamma*p*v[s_next]\n",
    "\n",
    "                v_collec[a] = v_loc\n",
    "                \n",
    "            \n",
    "            policy[s] = np.argmax(v_collec)\n",
    "            \n",
    "            \n",
    "        \n",
    "\n",
    "        if np.array_equal(policy_old,policy):\n",
    "            eff_ite = ite + 1\n",
    "            print('Total iterations needed for convergence were :',eff_ite)\n",
    "            break\n",
    "        \n",
    "        if ite == max_iterations - 1:\n",
    "            tot_ite = ite + 1\n",
    "            print('Total iterations, without convergence, were the maximum iterations :',tot_ite)\n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ########################### End of your code #########################\n",
    "    end = time.time()\n",
    "    print(\"Policy iteration took {0} seconds.\".format(end - start))\n",
    "    return policy\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    np.random.seed(1111)\n",
    "    env_name  = 'FrozenLake8x8-v0'\n",
    "    for gamma in [.9, .95, .99, .9999, 1]:\n",
    "        print(\"-\"*10, \"Gamma={0}\".format(gamma) ,\"-\"*10)\n",
    "        env = gym.make(env_name)\n",
    "        optimal_policy = policy_iteration(env, gamma=gamma)\n",
    "        scores = evaluate_policy(env, optimal_policy, gamma=gamma)\n",
    "        print('Average scores = ', np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions\n",
    "--- \n",
    "\n",
    "#### 1. How many iterations did it take for the value iteration to converge? How about policy iteration?\n",
    "Value iterations for me converged in:\n",
    "315, 495, 1128, 2337, 2357 \n",
    "Around 500-1500 iterations for Value iterations could thus be expected\n",
    "As gamma, increased no. of iterations needed for convergence also increased\n",
    "\n",
    "\n",
    "Policy iterations for me converged in:\n",
    "5,3,8,12,6 iterations\n",
    "Thus around 5-10 iterations were needed on average for the policy iteration algorithm to converge, however time consumed per iteration, as expected, was much larger.\n",
    "\n",
    "\n",
    "#### 2. How much time did it take for the value iteration to converge? How about the policy iteration?\n",
    "Value iterations converged in:\n",
    "0.36,0.63,1.43,2.85,3.00 seconds\n",
    "Again as gamma increased, time to converge also increased.\n",
    "\n",
    "Policy iterations converged in:\n",
    "0.2,0.23,1.36,4.2,2.44 seconds\n",
    "As such, time for policy iterations wasn't much smaller than value iteration despite the no. of value iterations being much much larger than the number of policy iterations\n",
    "\n",
    "\n",
    "#### 3. Which algorithm is faster? Why?\n",
    "Current run for me gave me results where both the algorithms were similar in terms of overall run times. However, as such, policy iterations generally win over value iterations, as discussed in the lecture, for complicated cases. Since, it also ends up giving the policy directly which the value iteration doesn't, policy iterations could be more favourable in general.\n",
    "\n",
    "Policy iterations are smaller in number than value iterations, however, policy iterations, per iteration, require more runtime than the value iteration. \n",
    "\n",
    "Since, the policy iteration converges with equality in finite steps, for complicated scenarios it could be picked since it could perform better in terms of speed\n",
    "\n",
    "\n",
    "#### 4. How does the average score change as $\\gamma$ gets closer to 1? Why?\n",
    "For Value iteration:\n",
    "As the gamma value increased, for me, the average score also increased and then at 1 it reached its maximum\n",
    "\n",
    "For Policy iteration:\n",
    "The trend here was also similar. As the value of gamma increased the average score increased.\n",
    "\n",
    "\n",
    "Reason: The overall reward function for each action and state, has, somewhat increased. Thus as gamma increases, one can expect the scores to increase, since overall each step reward also correspondingly increases with the increase in gamma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
